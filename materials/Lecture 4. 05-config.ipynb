{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://static.tildacdn.com/tild6236-6337-4339-b337-313363643735/new_logo.png\">\n",
    "\n",
    "# Настройка, мониторинг и оптимизация Spark\n",
    "**Андрей Титов**  \n",
    "tenke.iu8@gmail.com  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На этом занятии\n",
    "+ Настройка spark-submit окружения\n",
    "+ Работа со Spark UI\n",
    "+ \"Вредные\" советы\n",
    "\n",
    "## Настройка spark-submit окружения\n",
    "Дистрибутив Spark содержит в себе различные библиотеки, примеры конфигурационных файлов и набор утилит. Любое Spark приложение запускается одной из утилит:\n",
    "- spark-shell - запуска интерактивного `Scala REPL` с поддержкой Spark\n",
    "- pyspark - запуск интеракивного `python` шела с поддержкой Spark\n",
    "- spark-submit - запуск Spark приложений, собранных в виде `jar` или `py` файла с зависимостями\n",
    "\n",
    "### Структура дистрибутива Spark\n",
    "Скачаем последнюю версию с официального сайта:\n",
    "https://spark.apache.org/downloads.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@4df68113"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\n",
    "println(\"wget https://apache-mirror.rbc.ru/pub/apache/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz -P lib/\".!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распакуем `tar.gz` архив:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"tar xvf lib/spark-2.4.5-bin-hadoop2.7.tgz -C lib/\".!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Архив теперь можно удалить:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"rm -f lib/spark-2.4.5-bin-hadoop2.7.tgz\".!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изучим содержимое распакованного дистрибутива:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 152\n",
      "drwxr-xr-x   17 t3nq  staff   544B Oct 13 18:04 .\n",
      "drwxr-xr-x    3 t3nq  staff    96B Oct 13 18:04 ..\n",
      "-rw-r--r--    1 t3nq  staff    21K Feb  2  2020 LICENSE\n",
      "-rw-r--r--    1 t3nq  staff    42K Feb  2  2020 NOTICE\n",
      "drwxr-xr-x    3 t3nq  staff    96B Feb  2  2020 R\n",
      "-rw-r--r--    1 t3nq  staff   3.7K Feb  2  2020 README.md\n",
      "-rw-r--r--    1 t3nq  staff   187B Feb  2  2020 RELEASE\n",
      "drwxr-xr-x   29 t3nq  staff   928B Oct 13 18:04 bin\n",
      "drwxr-xr-x    9 t3nq  staff   288B Oct 13 19:02 conf\n",
      "drwxr-xr-x    5 t3nq  staff   160B Feb  2  2020 data\n",
      "drwxr-xr-x    4 t3nq  staff   128B Feb  2  2020 examples\n",
      "drwxr-xr-x  228 t3nq  staff   7.1K Oct 13 18:04 jars\n",
      "drwxr-xr-x    4 t3nq  staff   128B Feb  2  2020 kubernetes\n",
      "drwxr-xr-x   49 t3nq  staff   1.5K Oct 13 18:04 licenses\n",
      "drwxr-xr-x   19 t3nq  staff   608B Oct 13 18:04 python\n",
      "drwxr-xr-x   24 t3nq  staff   768B Oct 13 18:04 sbin\n",
      "drwxr-xr-x    3 t3nq  staff    96B Oct 13 18:04 yarn\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"ls -alh lib/spark-2.4.5-bin-hadoop2.7\".!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spark-submit`, `pyspark`, `spark-shell` находятся в каталоге `bin`. Когда вы запускаете свои Spark задачи, под капотом используются именно эти утилиты. Достаточно, чтобы они были установлены только на те хосты, с которых происходит запуск приложений. На обычных узлах кластера они не нужны. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 224\n",
      "drwxr-xr-x  29 t3nq  staff   928B Oct 13 18:04 .\n",
      "drwxr-xr-x  17 t3nq  staff   544B Oct 13 18:04 ..\n",
      "-rwxr-xr-x   1 t3nq  staff   1.1K Feb  2  2020 beeline\n",
      "-rw-r--r--   1 t3nq  staff   1.0K Feb  2  2020 beeline.cmd\n",
      "-rwxr-xr-x   1 t3nq  staff   5.3K Feb  2  2020 docker-image-tool.sh\n",
      "-rwxr-xr-x   1 t3nq  staff   1.9K Feb  2  2020 find-spark-home\n",
      "-rw-r--r--   1 t3nq  staff   2.6K Feb  2  2020 find-spark-home.cmd\n",
      "-rw-r--r--   1 t3nq  staff   1.8K Feb  2  2020 load-spark-env.cmd\n",
      "-rw-r--r--   1 t3nq  staff   2.0K Feb  2  2020 load-spark-env.sh\n",
      "-rwxr-xr-x   1 t3nq  staff   2.9K Feb  2  2020 pyspark\n",
      "-rw-r--r--   1 t3nq  staff   1.1K Feb  2  2020 pyspark.cmd\n",
      "-rw-r--r--   1 t3nq  staff   1.5K Feb  2  2020 pyspark2.cmd\n",
      "-rwxr-xr-x   1 t3nq  staff   1.0K Feb  2  2020 run-example\n",
      "-rw-r--r--   1 t3nq  staff   1.2K Feb  2  2020 run-example.cmd\n",
      "-rwxr-xr-x   1 t3nq  staff   3.1K Feb  2  2020 spark-class\n",
      "-rw-r--r--   1 t3nq  staff   1.2K Feb  2  2020 spark-class.cmd\n",
      "-rw-r--r--   1 t3nq  staff   2.8K Feb  2  2020 spark-class2.cmd\n",
      "-rwxr-xr-x   1 t3nq  staff   3.0K Feb  2  2020 spark-shell\n",
      "-rw-r--r--   1 t3nq  staff   1.2K Feb  2  2020 spark-shell.cmd\n",
      "-rw-r--r--   1 t3nq  staff   1.8K Feb  2  2020 spark-shell2.cmd\n",
      "-rwxr-xr-x   1 t3nq  staff   1.0K Feb  2  2020 spark-sql\n",
      "-rw-r--r--   1 t3nq  staff   1.1K Feb  2  2020 spark-sql.cmd\n",
      "-rw-r--r--   1 t3nq  staff   1.1K Feb  2  2020 spark-sql2.cmd\n",
      "-rwxr-xr-x   1 t3nq  staff   1.0K Feb  2  2020 spark-submit\n",
      "-rw-r--r--   1 t3nq  staff   1.2K Feb  2  2020 spark-submit.cmd\n",
      "-rw-r--r--   1 t3nq  staff   1.1K Feb  2  2020 spark-submit2.cmd\n",
      "-rwxr-xr-x   1 t3nq  staff   1.0K Feb  2  2020 sparkR\n",
      "-rw-r--r--   1 t3nq  staff   1.1K Feb  2  2020 sparkR.cmd\n",
      "-rw-r--r--   1 t3nq  staff   1.1K Feb  2  2020 sparkR2.cmd\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"ls -alh lib/spark-2.4.5-bin-hadoop2.7/bin\".!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В каталоге `python/lib` находятся зависимости для `python` - когда вы натраиваете у себя среду разработки, не нужно ставить pyspark в свой venv. Правильнее добавить эти библиотеки из дистрибутива. Так вы избежите возможных конфликтов версий в будущем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1256\n",
      "drwxr-xr-x   5 t3nq  staff   160B Oct 13 18:04 .\n",
      "drwxr-xr-x  19 t3nq  staff   608B Oct 13 18:04 ..\n",
      "-rw-r--r--   1 t3nq  staff   1.4K Feb  2  2020 PY4J_LICENSE.txt\n",
      "-rw-r--r--   1 t3nq  staff    41K Feb  2  2020 py4j-0.10.7-src.zip\n",
      "-rw-r--r--   1 t3nq  staff   578K Feb  2  2020 pyspark.zip\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"ls -alh lib/spark-2.4.5-bin-hadoop2.7/python/lib\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 471288\n",
      "drwxr-xr-x  228 t3nq  staff   7.1K Oct 13 18:04 .\n",
      "drwxr-xr-x   17 t3nq  staff   544B Oct 13 18:04 ..\n",
      "-rw-r--r--    1 t3nq  staff    17K Feb  2  2020 JavaEWAH-0.3.2.jar\n",
      "-rw-r--r--    1 t3nq  staff   318K Feb  2  2020 RoaringBitmap-0.7.45.jar\n",
      "-rw-r--r--    1 t3nq  staff   231K Feb  2  2020 ST4-4.0.4.jar\n",
      "-rw-r--r--    1 t3nq  staff    68K Feb  2  2020 activation-1.1.1.jar\n",
      "-rw-r--r--    1 t3nq  staff   131K Feb  2  2020 aircompressor-0.10.jar\n",
      "-rw-r--r--    1 t3nq  staff   435K Feb  2  2020 antlr-2.7.7.jar\n",
      "-rw-r--r--    1 t3nq  staff   161K Feb  2  2020 antlr-runtime-3.4.jar\n",
      "-rw-r--r--    1 t3nq  staff   327K Feb  2  2020 antlr4-runtime-4.7.jar\n",
      "-rw-r--r--    1 t3nq  staff   4.4K Feb  2  2020 aopalliance-1.0.jar\n",
      "-rw-r--r--    1 t3nq  staff    14K Feb  2  2020 aopalliance-repackaged-2.4.0-b34.jar\n",
      "-rw-r--r--    1 t3nq  staff   438K Feb  2  2020 apache-log4j-extras-1.2.17.jar\n",
      "-rw-r--r--    1 t3nq  staff    44K Feb  2  2020 apacheds-i18n-2.0.0-M15.jar\n",
      "-rw-r--r--    1 t3nq  staff   675K Feb  2  2020 apacheds-kerberos-codec-2.0.0-M15.jar\n",
      "-rw-r--r--    1 t3nq  staff    16K Feb  2  2020 api-asn1-api-1.0.0-M20.jar\n",
      "-rw-r--r--    1 t3nq  staff    78K Feb  2  2020 api-util-1.0.0-M20.jar\n",
      "-rw-r--r--    1 t3nq  staff   1.1M Feb  2  2020 arpack_combined_all-0.1.jar\n",
      "-rw-r--r--    1 t3nq  staff    51K Feb  2  2020 arrow-format-0.10.0.jar\n",
      "-rw-r--r--    1 t3nq  staff    77K Feb  2  2020 arrow-memory-0.10.0.jar\n",
      "-rw-r--r--    1 t3nq  staff   1.3M Feb  2  2020 arrow-vector-0.10.0.jar\n",
      "-rw-r--r--    1 t3nq  staff   172K Feb  2  2020 automaton-1.11-8.jar\n",
      "-rw-r--r--    1 t3nq  staff   1.5M Feb  2  2020 avro-1.8.2.jar\n",
      "-rw-r--r--    1 t3nq  staff   130K Feb  2  2020 avro-ipc-1.8.2.jar\n",
      "-rw-r--r--    1 t3nq  staff   183K Feb  2  2020 avro-mapred-1.8.2-hadoop2.jar\n",
      "-rw-r--r--    1 t3nq  staff   108K Feb  2  2020 bonecp-0.8.0.RELEASE.jar\n",
      "-rw-r--r--    1 t3nq  staff   183K Feb  2  2020 breeze-macros_2.11-0.13.2.jar\n",
      "-rw-r--r--    1 t3nq  staff    14M Feb  2  2020 breeze_2.11-0.13.2.jar\n",
      "-rw-r--r--    1 t3nq  staff   252K Feb  2  2020 calcite-avatica-1.2.0-incubating.jar\n",
      "-rw-r--r--    1 t3nq  staff   3.4M Feb  2  2020 calcite-core-1.2.0-incubating.jar\n",
      "-rw-r--r--    1 t3nq  staff   432K Feb  2  2020 calcite-linq4j-1.2.0-incubating.jar\n",
      "-rw-r--r--    1 t3nq  staff    57K Feb  2  2020 chill-java-0.9.3.jar\n",
      "-rw-r--r--    1 t3nq  staff   228K Feb  2  2020 chill_2.11-0.9.3.jar\n",
      "-rw-r--r--    1 t3nq  staff   241K Feb  2  2020 commons-beanutils-1.9.4.jar\n",
      "-rw-r--r--    1 t3nq  staff    40K Feb  2  2020 commons-cli-1.2.jar\n",
      "-rw-r--r--    1 t3nq  staff   278K Feb  2  2020 commons-codec-1.10.jar\n",
      "-rw-r--r--    1 t3nq  staff   575K Feb  2  2020 commons-collections-3.2.2.jar\n",
      "-rw-r--r--    1 t3nq  staff    37K Feb  2  2020 commons-compiler-3.0.9.jar\n",
      "-rw-r--r--    1 t3nq  staff   357K Feb  2  2020 commons-compress-1.8.1.jar\n",
      "-rw-r--r--    1 t3nq  staff   292K Feb  2  2020 commons-configuration-1.6.jar\n",
      "-rw-r--r--    1 t3nq  staff   131K Feb  2  2020 commons-crypto-1.0.0.jar\n",
      "-rw-r--r--    1 t3nq  staff   157K Feb  2  2020 commons-dbcp-1.4.jar\n",
      "-rw-r--r--    1 t3nq  staff   140K Feb  2  2020 commons-digester-1.8.jar\n",
      "-rw-r--r--    1 t3nq  staff   298K Feb  2  2020 commons-httpclient-3.1.jar\n",
      "-rw-r--r--    1 t3nq  staff   181K Feb  2  2020 commons-io-2.4.jar\n",
      "-rw-r--r--    1 t3nq  staff   278K Feb  2  2020 commons-lang-2.6.jar\n",
      "-rw-r--r--    1 t3nq  staff   469K Feb  2  2020 commons-lang3-3.5.jar\n",
      "-rw-r--r--    1 t3nq  staff    61K Feb  2  2020 commons-logging-1.1.3.jar\n",
      "-rw-r--r--    1 t3nq  staff   1.9M Feb  2  2020 commons-math3-3.4.1.jar\n",
      "-rw-r--r--    1 t3nq  staff   267K Feb  2  2020 commons-net-3.1.jar\n",
      "-rw-r--r--    1 t3nq  staff    94K Feb  2  2020 commons-pool-1.5.4.jar\n",
      "-rw-r--r--    1 t3nq  staff    78K Feb  2  2020 compress-lzf-1.0.3.jar\n",
      "-rw-r--r--    1 t3nq  staff   161K Feb  2  2020 core-1.1.2.jar\n",
      "-rw-r--r--    1 t3nq  staff    68K Feb  2  2020 curator-client-2.7.1.jar\n",
      "-rw-r--r--    1 t3nq  staff   182K Feb  2  2020 curator-framework-2.7.1.jar\n",
      "-rw-r--r--    1 t3nq  staff   264K Feb  2  2020 curator-recipes-2.7.1.jar\n",
      "-rw-r--r--    1 t3nq  staff   332K Feb  2  2020 datanucleus-api-jdo-3.2.6.jar\n",
      "-rw-r--r--    1 t3nq  staff   1.8M Feb  2  2020 datanucleus-core-3.2.10.jar\n",
      "-rw-r--r--    1 t3nq  staff   1.7M Feb  2  2020 datanucleus-rdbms-3.2.9.jar\n",
      "-rw-r--r--    1 t3nq  staff   3.1M Feb  2  2020 derby-10.12.1.1.jar\n",
      "-rw-r--r--    1 t3nq  staff    18K Feb  2  2020 eigenbase-properties-1.1.5.jar\n",
      "-rw-r--r--    1 t3nq  staff   9.9K Feb  2  2020 flatbuffers-1.2.0-3f79e055.jar\n",
      "-rw-r--r--    1 t3nq  staff    14K Feb  2  2020 generex-1.0.2.jar\n",
      "-rw-r--r--    1 t3nq  staff   186K Feb  2  2020 gson-2.2.4.jar\n",
      "-rw-r--r--    1 t3nq  staff   2.1M Feb  2  2020 guava-14.0.1.jar\n",
      "-rw-r--r--    1 t3nq  staff   694K Feb  2  2020 guice-3.0.jar\n",
      "-rw-r--r--    1 t3nq  staff    63K Feb  2  2020 guice-servlet-3.0.jar\n",
      "-rw-r--r--    1 t3nq  staff    40K Feb  2  2020 hadoop-annotations-2.7.3.jar\n",
      "-rw-r--r--    1 t3nq  staff    92K Feb  2  2020 hadoop-auth-2.7.3.jar\n",
      "-rw-r--r--    1 t3nq  staff    25K Feb  2  2020 hadoop-client-2.7.3.jar\n",
      "-rw-r--r--    1 t3nq  staff   3.3M Feb  2  2020 hadoop-common-2.7.3.jar\n",
      "-rw-r--r--    1 t3nq  staff   7.9M Feb  2  2020 hadoop-hdfs-2.7.3.jar\n",
      "-rw-r--r--    1 t3nq  staff   530K Feb  2  2020 hadoop-mapreduce-client-app-2.7.3.jar\n",
      "-rw-r--r--    1 t3nq  staff   758K Feb  2  2020 hadoop-mapreduce-client-common-2.7.3.jar\n",
      "-rw-r--r--    1 t3nq  staff   1.5M Feb  2  2020 hadoop-mapreduce-client-core-2.7.3.jar\n",
      "-rw-r--r--    1 t3nq  staff    61K Feb  2  2020 hadoop-mapreduce-client-jobclient-2.7.3.jar\n",
      "-rw-r--r--    1 t3nq  staff    70K Feb  2  2020 hadoop-mapreduce-client-shuffle-2.7.3.jar\n",
      "-rw-r--r--    1 t3nq  staff   1.9M Feb  2  2020 hadoop-yarn-api-2.7.3.jar\n",
      "-rw-r--r--    1 t3nq  staff   162K Feb  2  2020 hadoop-yarn-client-2.7.3.jar\n",
      "-rw-r--r--    1 t3nq  staff   1.6M Feb  2  2020 hadoop-yarn-common-2.7.3.jar\n",
      "-rw-r--r--    1 t3nq  staff   379K Feb  2  2020 hadoop-yarn-server-common-2.7.3.jar\n",
      "-rw-r--r--    1 t3nq  staff    57K Feb  2  2020 hadoop-yarn-server-web-proxy-2.7.3.jar\n",
      "-rw-r--r--    1 t3nq  staff   135K Feb  2  2020 hive-beeline-1.2.1.spark2.jar\n",
      "-rw-r--r--    1 t3nq  staff    40K Feb  2  2020 hive-cli-1.2.1.spark2.jar\n",
      "-rw-r--r--    1 t3nq  staff    11M Feb  2  2020 hive-exec-1.2.1.spark2.jar\n",
      "-rw-r--r--    1 t3nq  staff    98K Feb  2  2020 hive-jdbc-1.2.1.spark2.jar\n",
      "-rw-r--r--    1 t3nq  staff   5.3M Feb  2  2020 hive-metastore-1.2.1.spark2.jar\n",
      "-rw-r--r--    1 t3nq  staff   175K Feb  2  2020 hk2-api-2.4.0-b34.jar\n",
      "-rw-r--r--    1 t3nq  staff   177K Feb  2  2020 hk2-locator-2.4.0-b34.jar\n",
      "-rw-r--r--    1 t3nq  staff   116K Feb  2  2020 hk2-utils-2.4.0-b34.jar\n",
      "-rw-r--r--    1 t3nq  staff   1.6M Feb  2  2020 hppc-0.7.2.jar\n",
      "-rw-r--r--    1 t3nq  staff   1.4M Feb  2  2020 htrace-core-3.1.0-incubating.jar\n",
      "-rw-r--r--    1 t3nq  staff   749K Feb  2  2020 httpclient-4.5.6.jar\n",
      "-rw-r--r--    1 t3nq  staff   319K Feb  2  2020 httpcore-4.4.10.jar\n",
      "-rw-r--r--    1 t3nq  staff   1.2M Feb  2  2020 ivy-2.4.0.jar\n",
      "-rw-r--r--    1 t3nq  staff    46K Feb  2  2020 jackson-annotations-2.6.7.jar\n",
      "-rw-r--r--    1 t3nq  staff   253K Feb  2  2020 jackson-core-2.6.7.jar\n",
      "-rw-r--r--    1 t3nq  staff   227K Feb  2  2020 jackson-core-asl-1.9.13.jar\n",
      "-rw-r--r--    1 t3nq  staff   1.1M Feb  2  2020 jackson-databind-2.6.7.3.jar\n",
      "-rw-r--r--    1 t3nq  staff   313K Feb  2  2020 jackson-dataformat-yaml-2.6.7.jar\n",
      "-rw-r--r--    1 t3nq  staff    18K Feb  2  2020 jackson-jaxrs-1.9.13.jar\n",
      "-rw-r--r--    1 t3nq  staff   762K Feb  2  2020 jackson-mapper-asl-1.9.13.jar\n",
      "-rw-r--r--    1 t3nq  staff    32K Feb  2  2020 jackson-module-jaxb-annotations-2.6.7.jar\n",
      "-rw-r--r--    1 t3nq  staff    42K Feb  2  2020 jackson-module-paranamer-2.7.9.jar\n",
      "-rw-r--r--    1 t3nq  staff   504K Feb  2  2020 jackson-module-scala_2.11-2.6.7.1.jar\n",
      "-rw-r--r--    1 t3nq  staff    26K Feb  2  2020 jackson-xc-1.9.13.jar\n",
      "-rw-r--r--    1 t3nq  staff   783K Feb  2  2020 janino-3.0.9.jar\n",
      "-rw-r--r--    1 t3nq  staff   697K Feb  2  2020 javassist-3.18.1-GA.jar\n",
      "-rw-r--r--    1 t3nq  staff    26K Feb  2  2020 javax.annotation-api-1.2.jar\n",
      "-rw-r--r--    1 t3nq  staff   2.4K Feb  2  2020 javax.inject-1.jar\n",
      "-rw-r--r--    1 t3nq  staff   5.8K Feb  2  2020 javax.inject-2.4.0-b34.jar\n",
      "-rw-r--r--    1 t3nq  staff    94K Feb  2  2020 javax.servlet-api-3.1.0.jar\n",
      "-rw-r--r--    1 t3nq  staff   113K Feb  2  2020 javax.ws.rs-api-2.0.1.jar\n",
      "-rw-r--r--    1 t3nq  staff   386K Feb  2  2020 javolution-5.5.1.jar\n",
      "-rw-r--r--    1 t3nq  staff   103K Feb  2  2020 jaxb-api-2.2.2.jar\n",
      "-rw-r--r--    1 t3nq  staff    16K Feb  2  2020 jcl-over-slf4j-1.7.16.jar\n",
      "-rw-r--r--    1 t3nq  staff   196K Feb  2  2020 jdo-api-3.0.1.jar\n",
      "-rw-r--r--    1 t3nq  staff   163K Feb  2  2020 jersey-client-2.22.2.jar\n",
      "-rw-r--r--    1 t3nq  staff   682K Feb  2  2020 jersey-common-2.22.2.jar\n",
      "-rw-r--r--    1 t3nq  staff    18K Feb  2  2020 jersey-container-servlet-2.22.2.jar\n",
      "-rw-r--r--    1 t3nq  staff    65K Feb  2  2020 jersey-container-servlet-core-2.22.2.jar\n",
      "-rw-r--r--    1 t3nq  staff   949K Feb  2  2020 jersey-guava-2.22.2.jar\n",
      "-rw-r--r--    1 t3nq  staff    71K Feb  2  2020 jersey-media-jaxb-2.22.2.jar\n",
      "-rw-r--r--    1 t3nq  staff   929K Feb  2  2020 jersey-server-2.22.2.jar\n",
      "-rw-r--r--    1 t3nq  staff   527K Feb  2  2020 jetty-6.1.26.jar\n",
      "-rw-r--r--    1 t3nq  staff   173K Feb  2  2020 jetty-util-6.1.26.jar\n",
      "-rw-r--r--    1 t3nq  staff   262K Feb  2  2020 jline-2.14.6.jar\n",
      "-rw-r--r--    1 t3nq  staff   613K Feb  2  2020 joda-time-2.9.3.jar\n",
      "-rw-r--r--    1 t3nq  staff   418K Feb  2  2020 jodd-core-3.5.2.jar\n",
      "-rw-r--r--    1 t3nq  staff    12K Feb  2  2020 jpam-1.1.jar\n",
      "-rw-r--r--    1 t3nq  staff    89K Feb  2  2020 json4s-ast_2.11-3.5.3.jar\n",
      "-rw-r--r--    1 t3nq  staff   674K Feb  2  2020 json4s-core_2.11-3.5.3.jar\n",
      "-rw-r--r--    1 t3nq  staff    47K Feb  2  2020 json4s-jackson_2.11-3.5.3.jar\n",
      "-rw-r--r--    1 t3nq  staff   600K Feb  2  2020 json4s-scalap_2.11-3.5.3.jar\n",
      "-rw-r--r--    1 t3nq  staff    98K Feb  2  2020 jsp-api-2.1.jar\n",
      "-rw-r--r--    1 t3nq  staff    32K Feb  2  2020 jsr305-1.3.9.jar\n",
      "-rw-r--r--    1 t3nq  staff    15K Feb  2  2020 jta-1.1.jar\n",
      "-rw-r--r--    1 t3nq  staff   747K Feb  2  2020 jtransforms-2.4.0.jar\n",
      "-rw-r--r--    1 t3nq  staff   4.5K Feb  2  2020 jul-to-slf4j-1.7.16.jar\n",
      "-rw-r--r--    1 t3nq  staff   401K Feb  2  2020 kryo-shaded-4.0.2.jar\n",
      "-rw-r--r--    1 t3nq  staff   671K Feb  2  2020 kubernetes-client-4.6.1.jar\n",
      "-rw-r--r--    1 t3nq  staff    11M Feb  2  2020 kubernetes-model-4.6.1.jar\n",
      "-rw-r--r--    1 t3nq  staff   3.9K Feb  2  2020 kubernetes-model-common-4.6.1.jar\n",
      "-rw-r--r--    1 t3nq  staff   1.0M Feb  2  2020 leveldbjni-all-1.8.jar\n",
      "-rw-r--r--    1 t3nq  staff   306K Feb  2  2020 libfb303-0.9.3.jar\n",
      "-rw-r--r--    1 t3nq  staff   229K Feb  2  2020 libthrift-0.9.3.jar\n",
      "-rw-r--r--    1 t3nq  staff   478K Feb  2  2020 log4j-1.2.17.jar\n",
      "-rw-r--r--    1 t3nq  staff    12K Feb  2  2020 logging-interceptor-3.12.0.jar\n",
      "-rw-r--r--    1 t3nq  staff   361K Feb  2  2020 lz4-java-1.4.0.jar\n",
      "-rw-r--r--    1 t3nq  staff    34K Feb  2  2020 machinist_2.11-0.6.1.jar\n",
      "-rw-r--r--    1 t3nq  staff   3.1K Feb  2  2020 macro-compat_2.11-1.1.1.jar\n",
      "-rw-r--r--    1 t3nq  staff   7.0M Feb  2  2020 mesos-1.4.0-shaded-protobuf.jar\n",
      "-rw-r--r--    1 t3nq  staff   118K Feb  2  2020 metrics-core-3.1.5.jar\n",
      "-rw-r--r--    1 t3nq  staff    21K Feb  2  2020 metrics-graphite-3.1.5.jar\n",
      "-rw-r--r--    1 t3nq  staff    15K Feb  2  2020 metrics-json-3.1.5.jar\n",
      "-rw-r--r--    1 t3nq  staff    38K Feb  2  2020 metrics-jvm-3.1.5.jar\n",
      "-rw-r--r--    1 t3nq  staff   5.6K Feb  2  2020 minlog-1.3.0.jar\n",
      "-rw-r--r--    1 t3nq  staff   1.3M Feb  2  2020 netty-3.9.9.Final.jar\n",
      "-rw-r--r--    1 t3nq  staff   3.9M Feb  2  2020 netty-all-4.1.42.Final.jar\n",
      "-rw-r--r--    1 t3nq  staff    53K Feb  2  2020 objenesis-2.5.1.jar\n",
      "-rw-r--r--    1 t3nq  staff   413K Feb  2  2020 okhttp-3.12.0.jar\n",
      "-rw-r--r--    1 t3nq  staff    87K Feb  2  2020 okio-1.15.0.jar\n",
      "-rw-r--r--    1 t3nq  staff    19K Feb  2  2020 opencsv-2.3.jar\n",
      "-rw-r--r--    1 t3nq  staff   1.5M Feb  2  2020 orc-core-1.5.5-nohive.jar\n",
      "-rw-r--r--    1 t3nq  staff   793K Feb  2  2020 orc-mapreduce-1.5.5-nohive.jar\n",
      "-rw-r--r--    1 t3nq  staff    27K Feb  2  2020 orc-shims-1.5.5.jar\n",
      "-rw-r--r--    1 t3nq  staff    64K Feb  2  2020 oro-2.0.8.jar\n",
      "-rw-r--r--    1 t3nq  staff    20K Feb  2  2020 osgi-resource-locator-1.0.1.jar\n",
      "-rw-r--r--    1 t3nq  staff    34K Feb  2  2020 paranamer-2.8.jar\n",
      "-rw-r--r--    1 t3nq  staff   1.0M Feb  2  2020 parquet-column-1.10.1.jar\n",
      "-rw-r--r--    1 t3nq  staff    93K Feb  2  2020 parquet-common-1.10.1.jar\n",
      "-rw-r--r--    1 t3nq  staff   829K Feb  2  2020 parquet-encoding-1.10.1.jar\n",
      "-rw-r--r--    1 t3nq  staff   706K Feb  2  2020 parquet-format-2.4.0.jar\n",
      "-rw-r--r--    1 t3nq  staff   279K Feb  2  2020 parquet-hadoop-1.10.1.jar\n",
      "-rw-r--r--    1 t3nq  staff   2.7M Feb  2  2020 parquet-hadoop-bundle-1.6.0.jar\n",
      "-rw-r--r--    1 t3nq  staff   1.0M Feb  2  2020 parquet-jackson-1.10.1.jar\n",
      "-rw-r--r--    1 t3nq  staff   521K Feb  2  2020 protobuf-java-2.5.0.jar\n",
      "-rw-r--r--    1 t3nq  staff   120K Feb  2  2020 py4j-0.10.7.jar\n",
      "-rw-r--r--    1 t3nq  staff    93K Feb  2  2020 pyrolite-4.13.jar\n",
      "-rw-r--r--    1 t3nq  staff    15M Feb  2  2020 scala-compiler-2.11.12.jar\n",
      "-rw-r--r--    1 t3nq  staff   5.5M Feb  2  2020 scala-library-2.11.12.jar\n",
      "-rw-r--r--    1 t3nq  staff   461K Feb  2  2020 scala-parser-combinators_2.11-1.1.0.jar\n",
      "-rw-r--r--    1 t3nq  staff   4.4M Feb  2  2020 scala-reflect-2.11.12.jar\n",
      "-rw-r--r--    1 t3nq  staff   655K Feb  2  2020 scala-xml_2.11-1.0.5.jar\n",
      "-rw-r--r--    1 t3nq  staff   3.4M Feb  2  2020 shapeless_2.11-2.3.2.jar\n",
      "-rw-r--r--    1 t3nq  staff   3.9K Feb  2  2020 shims-0.7.45.jar\n",
      "-rw-r--r--    1 t3nq  staff    40K Feb  2  2020 slf4j-api-1.7.16.jar\n",
      "-rw-r--r--    1 t3nq  staff   9.7K Feb  2  2020 slf4j-log4j12-1.7.16.jar\n",
      "-rw-r--r--    1 t3nq  staff   263K Feb  2  2020 snakeyaml-1.15.jar\n",
      "-rw-r--r--    1 t3nq  staff    48K Feb  2  2020 snappy-0.2.jar\n",
      "-rw-r--r--    1 t3nq  staff   1.9M Feb  2  2020 snappy-java-1.1.7.3.jar\n",
      "-rw-r--r--    1 t3nq  staff   9.8M Feb  2  2020 spark-catalyst_2.11-2.4.5.jar\n",
      "-rw-r--r--    1 t3nq  staff    13M Feb  2  2020 spark-core_2.11-2.4.5.jar\n",
      "-rw-r--r--    1 t3nq  staff   692K Feb  2  2020 spark-graphx_2.11-2.4.5.jar\n",
      "-rw-r--r--    1 t3nq  staff   1.7M Feb  2  2020 spark-hive-thriftserver_2.11-2.4.5.jar\n",
      "-rw-r--r--    1 t3nq  staff   1.3M Feb  2  2020 spark-hive_2.11-2.4.5.jar\n",
      "-rw-r--r--    1 t3nq  staff   525K Feb  2  2020 spark-kubernetes_2.11-2.4.5.jar\n",
      "-rw-r--r--    1 t3nq  staff    56K Feb  2  2020 spark-kvstore_2.11-2.4.5.jar\n",
      "-rw-r--r--    1 t3nq  staff    74K Feb  2  2020 spark-launcher_2.11-2.4.5.jar\n",
      "-rw-r--r--    1 t3nq  staff   671K Feb  2  2020 spark-mesos_2.11-2.4.5.jar\n",
      "-rw-r--r--    1 t3nq  staff   180K Feb  2  2020 spark-mllib-local_2.11-2.4.5.jar\n",
      "-rw-r--r--    1 t3nq  staff   7.7M Feb  2  2020 spark-mllib_2.11-2.4.5.jar\n",
      "-rw-r--r--    1 t3nq  staff   2.3M Feb  2  2020 spark-network-common_2.11-2.4.5.jar\n",
      "-rw-r--r--    1 t3nq  staff    69K Feb  2  2020 spark-network-shuffle_2.11-2.4.5.jar\n",
      "-rw-r--r--    1 t3nq  staff   134K Feb  2  2020 spark-repl_2.11-2.4.5.jar\n",
      "-rw-r--r--    1 t3nq  staff    29K Feb  2  2020 spark-sketch_2.11-2.4.5.jar\n",
      "-rw-r--r--    1 t3nq  staff   9.4M Feb  2  2020 spark-sql_2.11-2.4.5.jar\n",
      "-rw-r--r--    1 t3nq  staff   2.1M Feb  2  2020 spark-streaming_2.11-2.4.5.jar\n",
      "-rw-r--r--    1 t3nq  staff   8.6K Feb  2  2020 spark-tags_2.11-2.4.5-tests.jar\n",
      "-rw-r--r--    1 t3nq  staff    15K Feb  2  2020 spark-tags_2.11-2.4.5.jar\n",
      "-rw-r--r--    1 t3nq  staff    49K Feb  2  2020 spark-unsafe_2.11-2.4.5.jar\n",
      "-rw-r--r--    1 t3nq  staff   659K Feb  2  2020 spark-yarn_2.11-2.4.5.jar\n",
      "-rw-r--r--    1 t3nq  staff    85K Feb  2  2020 spire-macros_2.11-0.13.0.jar\n",
      "-rw-r--r--    1 t3nq  staff   9.7M Feb  2  2020 spire_2.11-0.13.0.jar\n",
      "-rw-r--r--    1 t3nq  staff    23K Feb  2  2020 stax-api-1.0-2.jar\n",
      "-rw-r--r--    1 t3nq  staff    26K Feb  2  2020 stax-api-1.0.1.jar\n",
      "-rw-r--r--    1 t3nq  staff   170K Feb  2  2020 stream-2.7.0.jar\n",
      "-rw-r--r--    1 t3nq  staff   145K Feb  2  2020 stringtemplate-3.2.1.jar\n",
      "-rw-r--r--    1 t3nq  staff    91K Feb  2  2020 super-csv-2.2.0.jar\n",
      "-rw-r--r--    1 t3nq  staff   395K Feb  2  2020 univocity-parsers-2.7.3.jar\n",
      "-rw-r--r--    1 t3nq  staff    62K Feb  2  2020 validation-api-1.1.0.Final.jar\n",
      "-rw-r--r--    1 t3nq  staff   276K Feb  2  2020 xbean-asm6-shaded-4.8.jar\n",
      "-rw-r--r--    1 t3nq  staff   1.2M Feb  2  2020 xercesImpl-2.9.1.jar\n",
      "-rw-r--r--    1 t3nq  staff    15K Feb  2  2020 xmlenc-0.52.jar\n",
      "-rw-r--r--    1 t3nq  staff    97K Feb  2  2020 xz-1.5.jar\n",
      "-rw-r--r--    1 t3nq  staff    35K Feb  2  2020 zjsonpatch-0.3.0.jar\n",
      "-rw-r--r--    1 t3nq  staff   774K Feb  2  2020 zookeeper-3.4.6.jar\n",
      "-rw-r--r--    1 t3nq  staff   2.2M Feb  2  2020 zstd-jni-1.3.2-2.jar\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"ls -alh lib/spark-2.4.5-bin-hadoop2.7/jars\".!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данных `jar` находятся библиотеки, включая классы драйвера и воркеров:\n",
    "- org.apache.spark.repl.Main\n",
    "- org.apache.spark.deploy.SparkSubmit\n",
    "- org.apache.spark.executor.CoarseGrainedExecutorBackend\n",
    "\n",
    "```scala\n",
    "object SparkSubmit extends CommandLineUtils with Logging {\n",
    "  override def main(args: Array[String]): Unit\n",
    "}\n",
    "```\n",
    "\n",
    "```scala\n",
    "private[spark] object CoarseGrainedExecutorBackend extends Logging {\n",
    "    def main(args: Array[String])\n",
    "}\n",
    "```\n",
    "\n",
    "```scala\n",
    "object Main extends Logging {\n",
    "    def main(args: Array[String]): Unit\n",
    "}\n",
    "```\n",
    "\n",
    "Также в дистрибутив входят библиотеки для работы с Hadoop, встроенными источниками данных, например Parquet, и т. д.\n",
    "\n",
    "В каталоге `conf` находятся шаблоны конфигурационных файлов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 72\n",
      "drwxr-xr-x   9 t3nq  staff   288B Oct 13 19:02 .\n",
      "drwxr-xr-x  17 t3nq  staff   544B Oct 13 18:04 ..\n",
      "-rw-r--r--   1 t3nq  staff   996B Feb  2  2020 docker.properties.template\n",
      "-rw-r--r--   1 t3nq  staff   1.1K Feb  2  2020 fairscheduler.xml.template\n",
      "-rw-r--r--   1 t3nq  staff   2.0K Feb  2  2020 log4j.properties.template\n",
      "-rw-r--r--   1 t3nq  staff   7.6K Feb  2  2020 metrics.properties.template\n",
      "-rw-r--r--   1 t3nq  staff   865B Feb  2  2020 slaves.template\n",
      "-rw-r--r--   1 t3nq  staff   1.3K Feb  2  2020 spark-defaults.conf.template\n",
      "-rwxr-xr-x   1 t3nq  staff   4.1K Feb  2  2020 spark-env.sh.template\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"ls -alh lib/spark-2.4.5-bin-hadoop2.7/conf\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
      "# contributor license agreements.  See the NOTICE file distributed with\n",
      "# this work for additional information regarding copyright ownership.\n",
      "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
      "# (the \"License\"); you may not use this file except in compliance with\n",
      "# the License.  You may obtain a copy of the License at\n",
      "#\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "#\n",
      "\n",
      "# Set everything to be logged to the console\n",
      "log4j.rootCategory=INFO, console\n",
      "log4j.appender.console=org.apache.log4j.ConsoleAppender\n",
      "log4j.appender.console.target=System.err\n",
      "log4j.appender.console.layout=org.apache.log4j.PatternLayout\n",
      "log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n",
      "\n",
      "# Set the default spark-shell log level to WARN. When running the spark-shell, the\n",
      "# log level for this class is used to overwrite the root logger's log level, so that\n",
      "# the user can have different defaults for the shell and regular Spark apps.\n",
      "log4j.logger.org.apache.spark.repl.Main=WARN\n",
      "\n",
      "# Settings to quiet third party logs that are too verbose\n",
      "log4j.logger.org.spark_project.jetty=WARN\n",
      "log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR\n",
      "log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\n",
      "log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n",
      "log4j.logger.org.apache.parquet=ERROR\n",
      "log4j.logger.parquet=ERROR\n",
      "\n",
      "# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support\n",
      "log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL\n",
      "log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"cat lib/spark-2.4.5-bin-hadoop2.7/conf/log4j.properties.template\".!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spark-defaults.conf**  \n",
    "Основной конфигурационный файл Spark. В нем указываются опции запуска, зависимости, количество воркеров и т. п.\n",
    "\n",
    "**spark-env.sh**  \n",
    "Скрипт, в котором устанавливаются переменные окружения (например, `HADOOP_CONF_DIR` или `YARN_CONF_DIR`)\n",
    "\n",
    "**log4j.properties**\n",
    "Конфигурация логгеров - здесь можно задать необходимый уровень логирования для различных компонентов Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изучим `spark-submit --help`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"lib/spark-2.4.5-bin-hadoop2.7/bin/spark-submit --help\".!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приоритет опций (по уменьшению приоритета):\n",
    "- параметры `spark-submit`\n",
    "- переменные окружения\n",
    "- конфигурационный файл `spark-defaults.conf`\n",
    "\n",
    "### Выводы:\n",
    "- Наличие Hadoop не является необходимым условием для работы Spark\n",
    "- В Spark есть большое количество параметров, определяющих режим его работы. Большинство описано здесь: https://spark.apache.org/docs/latest/configuration.html\n",
    "- Утилита `spark-submit` позволяет запустить Spark приложение как локально, так и на кластере\n",
    "\n",
    "## Работа со Spark UI\n",
    "\n",
    "Каждое Spark приложение по умолчанию поднимает UI, который позволяет изучить состояние задачи и провести диагностику производительности. Данные, представленные в UI, также можно получить через REST API\n",
    "\n",
    "Получить Spark UI URL можно следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sparkUiUrl: Option[String] = spark.sparkContext.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "sparkUiUrl.foreach( x => println(s\"curl -s $x/api/v1/applications\".!!))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val appId = spark.sparkContext.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkUiUrl.foreach( x => println(s\"curl -s $x/api/v1/applications/$appId\".!!))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список доступных методов доступен по ссылке: https://spark.apache.org/docs/latest/monitoring.html#rest-api\n",
    "\n",
    "Прочитаем датасет [Airport Codes](https://datahub.io/core/airport-codes):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вредные советы\n",
    "В данной секции приведены частые ошибки, которые допускаются при работе с данными в DataFrame API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "-RECORD 0------------------------------------------\n",
      " ident        | 00A                                \n",
      " type         | heliport                           \n",
      " name         | Total Rf Heliport                  \n",
      " elevation_ft | 11                                 \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-PA                              \n",
      " municipality | Bensalem                           \n",
      " gps_code     | 00A                                \n",
      " iata_code    | null                               \n",
      " local_code   | 00A                                \n",
      " coordinates  | -74.93360137939453, 40.07080078125 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "csvOptions = Map(header -> true, inferSchema -> true)\n",
       "airports = [ident: string, type: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 10 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Эта нормальная :)\n",
    "val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.options(csvOptions).csv(\"/tmp/datasets/airport-codes.csv\")\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Window funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [ident#10, iso_country#15, elevation_ft#13, type#11, 56226 AS cnt#267L]\n",
      "+- *(1) FileScan csv [ident#10,type#11,elevation_ft#13,iso_country#15] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tmp/datasets/airport-codes.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ident:string,type:string,elevation_ft:int,iso_country:string>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cnt = 56226\n",
       "ranked = [ident: string, iso_country: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, iso_country: string ... 3 more fields]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// val wnd = Window.partitionBy()\n",
    "\n",
    "// val ranked = airports.select('ident, 'iso_country, 'elevation_ft, 'type, count(lit(1)).over(wnd))\n",
    "// ranked.show(20, false)\n",
    "// ranked.explain\n",
    "\n",
    "val cnt = airports.count\n",
    "val ranked = airports.select('ident, 'iso_country, 'elevation_ft, 'type, lit(cnt).alias(\"cnt\"))\n",
    "ranked.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Sort [height#321 ASC NULLS FIRST], false, 0\n",
      "+- Exchange RoundRobinPartitioning(1)\n",
      "   +- *(2) HashAggregate(keys=[iso_country#15], functions=[max(elevation_ft#13)])\n",
      "      +- Exchange hashpartitioning(iso_country#15, 200)\n",
      "         +- *(1) HashAggregate(keys=[iso_country#15], functions=[partial_max(elevation_ft#13)])\n",
      "            +- *(1) FileScan csv [elevation_ft#13,iso_country#15] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tmp/datasets/airport-codes.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<elevation_ft:int,iso_country:string>\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ret = [iso_country: string, height: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[iso_country: string, height: int]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ret = \n",
    "    airports\n",
    "        .groupBy('iso_country)\n",
    "        .agg(max('elevation_ft).alias(\"height\"))\n",
    "        .repartition(1)\n",
    "        .sortWithinPartitions('height.asc)\n",
    "\n",
    "ret.write.mode(\"ignore\").parquet(\"/tmp/datasets/out\")\n",
    "ret.explain\n",
    "println(ret.rdd.getNumPartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Exchange RoundRobinPartitioning(1)\n",
      "+- *(1) Range (0, 10, step=1, splits=10)\n"
     ]
    }
   ],
   "source": [
    "spark.range(0, 10, 1, 10).repartition(1).explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|                                             value|\n",
      "+--------------------------------------------------+\n",
      "|{\"ident\":\"00A\",\"type\":\"heliport\",\"name\":\"Total ...|\n",
      "|{\"ident\":\"00AA\",\"type\":\"small_airport\",\"name\":\"...|\n",
      "|{\"ident\":\"00AK\",\"type\":\"small_airport\",\"name\":\"...|\n",
      "|{\"ident\":\"00AL\",\"type\":\"small_airport\",\"name\":\"...|\n",
      "|{\"ident\":\"00AR\",\"type\":\"closed\",\"name\":\"Newport...|\n",
      "|{\"ident\":\"00AS\",\"type\":\"small_airport\",\"name\":\"...|\n",
      "|{\"ident\":\"00AZ\",\"type\":\"small_airport\",\"name\":\"...|\n",
      "|{\"ident\":\"00CA\",\"type\":\"small_airport\",\"name\":\"...|\n",
      "|{\"ident\":\"00CL\",\"type\":\"small_airport\",\"name\":\"...|\n",
      "|{\"ident\":\"00CN\",\"type\":\"heliport\",\"name\":\"Kitch...|\n",
      "+--------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true, false) AS value#335]\n",
      "+- MapPartitions <function1>, obj#334: java.lang.String\n",
      "   +- DeserializeToObject createexternalrow(ident#10.toString, type#11.toString, name#12.toString, elevation_ft#13, continent#14.toString, iso_country#15.toString, iso_region#16.toString, municipality#17.toString, gps_code#18.toString, iata_code#19.toString, local_code#20.toString, coordinates#21.toString, StructField(ident,StringType,true), StructField(type,StringType,true), StructField(name,StringType,true), StructField(elevation_ft,IntegerType,true), StructField(continent,StringType,true), StructField(iso_country,StringType,true), StructField(iso_region,StringType,true), StructField(municipality,StringType,true), StructField(gps_code,StringType,true), StructField(iata_code,StringType,true), StructField(local_code,StringType,true), StructField(coordinates,StringType,true)), obj#333: org.apache.spark.sql.Row\n",
      "      +- *(1) FileScan csv [ident#10,type#11,name#12,elevation_ft#13,continent#14,iso_country#15,iso_region#16,municipality#17,gps_code#18,iata_code#19,local_code#20,coordinates#21] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tmp/datasets/airport-codes.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_country:string,...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "jsoned = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jsoned = airports.toJSON\n",
    "jsoned.show(10, 50)\n",
    "jsoned.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toJSON: [T](ds: org.apache.spark.sql.Dataset[T])org.apache.spark.sql.Dataset[String]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Column\n",
    "\n",
    "def toJSON[T](ds: Dataset[T]): Dataset[String] = {\n",
    "    val foo: Column = to_json(struct(ds.columns.map(x => col(x)):_*))\n",
    "    ds.select(foo).as[String]\n",
    "}\n",
    "\n",
    "// def\n",
    "// struct(cols: Column*): Column\n",
    "//  Permalink\n",
    "// Creates a new struct column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Project [structstojson(named_struct(ident, ident#10, type, type#11, name, name#12, elevation_ft, elevation_ft#13, continent, continent#14, iso_country, iso_country#15, iso_region, iso_region#16, municipality, municipality#17, gps_code, gps_code#18, iata_code, iata_code#19, local_code, local_code#20, coordinates, coordinates#21), Some(Europe/Moscow)) AS structstojson(named_struct(NamePlaceholder(), ident, NamePlaceholder(), type, NamePlaceholder(), name, NamePlaceholder(), elevation_ft, NamePlaceholder(), continent, NamePlaceholder(), iso_country, NamePlaceholder(), iso_region, NamePlaceholder(), municipality, NamePlaceholder(), gps_code, NamePlaceholder(), iata_code, NamePlaceholder(), local_code, NamePlaceholder(), coordinates))#365]\n",
      "+- *(1) FileScan csv [ident#10,type#11,name#12,elevation_ft#13,continent#14,iso_country#15,iso_region#16,municipality#17,gps_code#18,iata_code#19,local_code#20,coordinates#21] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tmp/datasets/airport-codes.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_country:string,...\n"
     ]
    }
   ],
   "source": [
    "toJSON(airports).explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|structstojson(named_struct(NamePlaceholder(), id))|\n",
      "+--------------------------------------------------+\n",
      "|                                          {\"id\":0}|\n",
      "|                                          {\"id\":1}|\n",
      "|                                          {\"id\":2}|\n",
      "|                                          {\"id\":3}|\n",
      "|                                          {\"id\":4}|\n",
      "|                                          {\"id\":5}|\n",
      "|                                          {\"id\":6}|\n",
      "|                                          {\"id\":7}|\n",
      "|                                          {\"id\":8}|\n",
      "|                                          {\"id\":9}|\n",
      "+--------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [id: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.range(10)\n",
    "df.select(to_json(struct(df.columns.map(x => col(x)):_*))).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Apple\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Apple(size: Int, color: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "LocalTableScan [size#395]\n"
     ]
    }
   ],
   "source": [
    "List(Apple(1, \"red\")).toDS.select('size).explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mega_udf = UserDefinedFunction(<function2>,StringType,Some(List(IntegerType, StringType)))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function2>,StringType,Some(List(IntegerType, StringType)))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mega_udf = udf { (left: java.lang.Integer, right: String) => \"ok\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|UDF(left, right)|\n",
      "+----------------+\n",
      "|              ok|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark\n",
    "    .range(1)\n",
    "    .select(\n",
    "        lit(1).alias(\"left\"), \n",
    "        lit(\"foo\").alias(\"right\")\n",
    "    )\n",
    "    .select(mega_udf('left, 'right))\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|UDF(left, right)|\n",
      "+----------------+\n",
      "|              ok|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark\n",
    "    .range(1)\n",
    "    .select(\n",
    "        lit(null).alias(\"left\"), \n",
    "        lit(\"foo\").alias(\"right\")\n",
    "    )\n",
    "    .select(mega_udf('left, 'right))\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|UDF(left, right)|\n",
      "+----------------+\n",
      "|              ok|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark\n",
    "    .range(1)\n",
    "    .select(\n",
    "        lit(1).alias(\"left\"), \n",
    "        lit(null).alias(\"right\")\n",
    "    )\n",
    "    .select(mega_udf('left, 'right))\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foo = 3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val foo: java.lang.Integer = 3\n",
    "foo.toInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Foo\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Foo(first: Int, second: Int, third: Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mega_udf2 = UserDefinedFunction(<function0>,StructType(StructField(first,IntegerType,false), StructField(second,IntegerType,false), StructField(third,IntegerType,false)),Some(List()))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function0>,StructType(StructField(first,IntegerType,false), StructField(second,IntegerType,false), StructField(third,IntegerType,false)),Some(List()))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mega_udf2 = udf { () => Thread.sleep(1000); Foo(1,2,3) }.asNondeterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [res#576.first AS res.first#602, res#576.second AS res.second#603, res#576.third AS res.third#604]\n",
      "+- InMemoryTableScan [res#576]\n",
      "      +- InMemoryRelation [res#576], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "            +- *(1) Project [UDF() AS res#576]\n",
      "               +- *(1) Range (0, 10, step=1, splits=1)\n",
      "Time taken: 10409 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[res: struct<first: int, second: int ... 1 more field>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.time { \n",
    "val df = spark\n",
    "    .range(0, 10, 1, 1)\n",
    "    .select(mega_udf2().alias(\"res\"))\n",
    "df.cache\n",
    "df.count\n",
    "df.select('res(\"first\"), 'res(\"second\"), 'res(\"third\")).explain\n",
    "df.unpersist\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|first|second|third|\n",
      "+-----+------+-----+\n",
      "|    1|     2|    3|\n",
      "|    1|     2|    3|\n",
      "|    1|     2|    3|\n",
      "|    1|     2|    3|\n",
      "|    1|     2|    3|\n",
      "|    1|     2|    3|\n",
      "|    1|     2|    3|\n",
      "|    1|     2|    3|\n",
      "|    1|     2|    3|\n",
      "|    1|     2|    3|\n",
      "+-----+------+-----+\n",
      "\n",
      "Time taken: 10133 ms\n"
     ]
    }
   ],
   "source": [
    "spark.time { \n",
    "spark\n",
    "    .range(0, 10, 1, 1)\n",
    "    .select(mega_udf2().alias(\"res\"))\n",
    "    .select(col(\"res.*\")).show\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys.process._\n",
    "\"\"\"cp -f /tmp/datasets/source/1.txt /tmp/datasets/cache.txt/1.txt\"\"\".!\n",
    "\"\"\"cp -f /tmp/datasets/source/2.txt /tmp/datasets/cache.txt/2.txt\"\"\".!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Some(http://192.168.1.68:4042)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    3|\n",
      "|    3|\n",
      "|    3|\n",
      "|    3|\n",
      "|    3|\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.text(\"/tmp/datasets/cache.txt\")\n",
    "df.show(10)\n",
    "\n",
    "df.show(1)\n",
    "df.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"cp -f /tmp/datasets/source/1.txt /tmp/datasets/cache.txt/2.txt\"\"\".!\n",
    "\"\"\"cp -f /tmp/datasets/source/3.txt /tmp/datasets/cache.txt/1.txt\"\"\".!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    2|\n",
      "|    2|\n",
      "|    2|\n",
      "|    2|\n",
      "|    2|\n",
      "|    3|\n",
      "|    3|\n",
      "|    3|\n",
      "|    3|\n",
      "|    3|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.text(\"/tmp/datasets/cache.txt\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sharedState.cacheManager.clearCache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mega_udf2 = UserDefinedFunction(<function0>,StructType(StructField(first,IntegerType,false), StructField(second,IntegerType,false), StructField(third,IntegerType,false)),Some(List()))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function0>,StructType(StructField(first,IntegerType,false), StructField(second,IntegerType,false), StructField(third,IntegerType,false)),Some(List()))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mega_udf2 = udf { () => Thread.sleep(1000); Foo(1,2,3) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|      foo|\n",
      "+---+---------+\n",
      "|  0|[1, 2, 3]|\n",
      "|  1|[1, 2, 3]|\n",
      "|  2|[1, 2, 3]|\n",
      "|  3|[1, 2, 3]|\n",
      "|  4|[1, 2, 3]|\n",
      "|  5|[1, 2, 3]|\n",
      "|  6|[1, 2, 3]|\n",
      "|  7|[1, 2, 3]|\n",
      "|  8|[1, 2, 3]|\n",
      "|  9|[1, 2, 3]|\n",
      "+---+---------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [id#864L, UDF() AS foo#866]\n",
      "+- *(1) Range (0, 10, step=1, splits=4)\n",
      "Time taken: 5113 ms\n"
     ]
    }
   ],
   "source": [
    "spark.time {\n",
    "val ret = spark.range(0, 10, 1, 4).withColumn(\"foo\", mega_udf2())\n",
    "ret.show\n",
    "ret.explain\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|      foo|\n",
      "+---+---------+\n",
      "|  0|[1, 2, 3]|\n",
      "|  1|[1, 2, 3]|\n",
      "|  2|[1, 2, 3]|\n",
      "|  3|[1, 2, 3]|\n",
      "|  4|[1, 2, 3]|\n",
      "|  5|[1, 2, 3]|\n",
      "|  6|[1, 2, 3]|\n",
      "|  7|[1, 2, 3]|\n",
      "|  8|[1, 2, 3]|\n",
      "|  9|[1, 2, 3]|\n",
      "+---+---------+\n",
      "\n",
      "== Physical Plan ==\n",
      "Coalesce 1\n",
      "+- InMemoryTableScan [id#709L, foo#711]\n",
      "      +- InMemoryRelation [id#709L, foo#711], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "            +- *(1) Project [id#709L, UDF() AS foo#711]\n",
      "               +- *(1) Range (0, 10, step=1, splits=2)\n",
      "Time taken: 5225 ms\n"
     ]
    }
   ],
   "source": [
    "spark.time {\n",
    "val ret = spark.range(0, 10, 1, 2).withColumn(\"foo\", mega_udf2())\n",
    "ret.cache\n",
    "ret.count\n",
    "ret.coalesce(1).show\n",
    "ret.coalesce(1).explain\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sharedState.cacheManager.clearCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
